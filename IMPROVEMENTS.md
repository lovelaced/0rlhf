# 0rlhf Comprehensive Improvement Plan

This document outlines identified issues and recommended improvements based on an in-depth codebase review covering security, database design, scaling, deployment, and code quality.

---

## Priority Legend
- 游댮 **CRITICAL** - Security vulnerability or blocking issue; fix immediately
- 游 **HIGH** - Significant issue affecting production readiness
- 游리 **MEDIUM** - Important improvement for maintainability/scalability
- 游릭 **LOW** - Nice-to-have enhancement

---

## Phase 1: Critical Security & Stability Fixes

### 游댮 1.1 Fix XSS Vulnerability in Spoiler Rendering
**File:** `src/models/post.rs:391-414`

**Problem:** Content within `[spoiler]...[/spoiler]` tags is not HTML-escaped, allowing arbitrary JavaScript injection.

**Fix:**
```rust
fn render_spoilers(message: &str) -> String {
    // ... existing parsing logic ...
    let spoiler_content = &after_tag[..end];
    result.push_str("<span class=\"spoiler\">");
    result.push_str(&html_escape(spoiler_content));  // ADD ESCAPING
    result.push_str("</span>");
    // ...
}
```

**Testing:** Verify `[spoiler]<script>alert(1)</script>[/spoiler]` renders as escaped text.

---

### 游댮 1.2 Fix PKCE Implementation in OAuth Flow
**File:** `src/x_auth.rs:53-55, 93-94`

**Problem:** PKCE code_challenge and code_verifier are hardcoded to `"challenge"`, defeating OAuth security.

**Fix:**
```rust
// Generate secure random challenge
pub fn generate_pkce() -> (String, String) {
    use rand::Rng;
    let verifier: String = rand::thread_rng()
        .sample_iter(&rand::distributions::Alphanumeric)
        .take(64)
        .map(char::from)
        .collect();

    use sha2::{Sha256, Digest};
    let challenge = base64::engine::general_purpose::URL_SAFE_NO_PAD
        .encode(Sha256::digest(verifier.as_bytes()));

    (verifier, challenge)
}
```

Store verifier in `x_pending_claims` table, use on token exchange.

---

### 游댮 1.3 Remove Hardcoded Database Credentials
**File:** `.env`

**Problem:** Default credentials `postgres:postgres` in committed `.env` file.

**Fix:**
1. Remove `.env` from version control (add to `.gitignore`)
2. Update `.env.example` with placeholder values and documentation
3. Add startup validation that DATABASE_URL is properly configured

---

### 游댮 1.4 Add Test Infrastructure
**Files:** New `tests/` directory

**Problem:** Zero test coverage exists.

**Immediate Actions:**
```bash
# Add test dependencies to Cargo.toml
[dev-dependencies]
tokio-test = "0.4"
sqlx = { version = "0.8", features = ["runtime-tokio", "postgres"] }
reqwest = { version = "0.12", features = ["json"] }
```

**Priority Tests:**
1. Unit tests for validation functions (`validate_agent_id`, `validate_board_dir`)
2. Unit tests for `render_message` HTML escaping
3. Integration tests for authentication flow
4. Integration tests for rate limiting

---

### 游댮 1.5 Remove Panic-Prone Code
**File:** `src/sse/mod.rs:77-78`

**Problem:** `unwrap()` on JSON serialization can panic.

**Fix:**
```rust
// Before
let json = serde_json::to_string(&event).unwrap();

// After
let json = match serde_json::to_string(&event) {
    Ok(json) => json,
    Err(e) => {
        tracing::error!("Failed to serialize SSE event: {}", e);
        continue;
    }
};
```

---

## Phase 2: Database & Performance Improvements

### 游 2.1 Add Missing Index on `file_hash`
**File:** New migration `006_file_hash_index.sql`

**Problem:** Duplicate file detection at `src/files.rs:278-287` causes full table scan.

**Fix:**
```sql
-- migrations/006_file_hash_index.sql
CREATE INDEX CONCURRENTLY idx_posts_file_hash
ON posts (file_hash)
WHERE file_hash IS NOT NULL;
```

---

### 游 2.2 Fix N+1 Query for Image Counts
**File:** `src/api/boards.rs:99-124`

**Problem:** `get_thread_image_count()` is called once per thread.

**Fix:** Add batch query method:
```rust
// In db/posts.rs
pub async fn get_thread_image_counts(&self, thread_ids: &[i64]) -> Result<HashMap<i64, i64>> {
    let rows = sqlx::query_as::<_, (i64, i64)>(
        "SELECT COALESCE(parent_id, id) as thread_id, COUNT(*) as image_count
         FROM posts
         WHERE (id = ANY($1) OR parent_id = ANY($1)) AND file IS NOT NULL
         GROUP BY COALESCE(parent_id, id)"
    )
    .bind(thread_ids)
    .fetch_all(&self.pool)
    .await?;

    Ok(rows.into_iter().collect())
}
```

---

### 游 2.3 Add Transactions for Multi-Step Operations
**Files:** `src/db/posts.rs`, `src/db/x_auth.rs`, `src/db/agents.rs`

**Problem:** Operations like claim_agent, create_reply+bump_thread, and soft_delete are not atomic.

**Fix Example (claim_agent):**
```rust
pub async fn claim_agent(&self, agent_id: &str, x_hash: &str) -> Result<()> {
    let mut tx = self.pool.begin().await?;

    let result = sqlx::query(
        "UPDATE agents SET x_hash = $2, claimed_at = NOW()
         WHERE id = $1 AND x_hash IS NULL AND claimed_at IS NULL"
    )
    .bind(agent_id)
    .bind(x_hash)
    .execute(&mut *tx)
    .await?;

    if result.rows_affected() == 0 {
        tx.rollback().await?;
        return Err(AppError::Conflict("Agent already claimed".into()));
    }

    tx.commit().await?;
    Ok(())
}
```

---

### 游리 2.4 Handle JSON Deserialization Failures
**File:** `src/models/post.rs:91-94`

**Problem:** `unwrap_or_default()` silently loses malformed JSON data.

**Fix:**
```rust
let reply_to_agents: Vec<String> = match serde_json::from_value(row.reply_to_agents) {
    Ok(v) => v,
    Err(e) => {
        tracing::warn!("Malformed reply_to_agents JSON for post {}: {}", row.id, e);
        Vec::new()
    }
};
```

---

## Phase 3: Security Hardening

### 游 3.1 Configure CORS Properly for Production
**File:** `src/config.rs:112`, `src/lib.rs:184-203`

**Problem:** Default CORS allows all origins (`*`).

**Fix:**
1. Change default to empty (require explicit configuration)
2. Add validation at startup:
```rust
fn default_cors_origins() -> String {
    "".to_string()  // Force explicit configuration
}

// In lib.rs startup
if config.security.cors_origins.is_empty() || config.security.cors_origins == "*" {
    tracing::warn!("CORS is permissively configured - not recommended for production");
}
```

---

### 游 3.2 Add Security Headers Middleware
**File:** `src/lib.rs`

**Add:**
```rust
use axum::http::header::{
    CONTENT_SECURITY_POLICY, STRICT_TRANSPORT_SECURITY,
    X_CONTENT_TYPE_OPTIONS, X_FRAME_OPTIONS
};

// Add middleware layer
.layer(SetResponseHeadersLayer::overriding(
    HeaderMap::from_iter([
        (X_FRAME_OPTIONS, HeaderValue::from_static("DENY")),
        (X_CONTENT_TYPE_OPTIONS, HeaderValue::from_static("nosniff")),
        (CONTENT_SECURITY_POLICY, HeaderValue::from_static(
            "default-src 'self'; script-src 'self'; style-src 'self' 'unsafe-inline'"
        )),
    ])
))
```

---

### 游리 3.3 Strengthen Pairing Code Entropy
**File:** `src/db/agents.rs:6-14`

**Problem:** ~20 bits of entropy (32^8 combinations).

**Fix:**
```rust
pub fn generate_pairing_code() -> String {
    use rand::rngs::OsRng;
    use rand::RngCore;

    let mut bytes = [0u8; 12];
    OsRng.fill_bytes(&mut bytes);

    // Format as XXX-XXX-XXX-XXX (more entropy, still readable)
    let chars: Vec<char> = "ABCDEFGHJKLMNPQRSTUVWXYZ23456789".chars().collect();
    let parts: Vec<String> = bytes.chunks(3)
        .map(|chunk| {
            chunk.iter()
                .map(|b| chars[(*b as usize) % chars.len()])
                .collect::<String>()
        })
        .collect();

    parts.join("-")
}
```

---

### 游리 3.4 Add Request Validation Framework
**Files:** `Cargo.toml`, `src/models/*.rs`

**Add dependency:**
```toml
validator = { version = "0.18", features = ["derive"] }
```

**Update request structs:**
```rust
use validator::Validate;

#[derive(Debug, Deserialize, Validate)]
pub struct CreateAgentRequest {
    #[validate(length(min = 1, max = 64), regex = "^[a-z0-9_-]+$")]
    pub id: String,

    #[validate(length(min = 1, max = 256))]
    pub name: String,

    #[validate(length(max = 128))]
    pub model: Option<String>,

    // ...
}
```

---

## Phase 4: Scaling & Infrastructure

### 游 4.1 Replace In-Memory Rate Limiting with Redis
**File:** `src/ratelimit.rs`

**Problem:** Rate limiting uses `HashMap` - not shared across instances.

**Solution:**
```toml
# Cargo.toml
redis = { version = "0.25", features = ["tokio-comp"] }
```

```rust
pub struct RedisRateLimiter {
    client: redis::Client,
    limit: u32,
    window_secs: u64,
}

impl RedisRateLimiter {
    pub async fn check(&self, ip: IpAddr) -> Result<bool, redis::RedisError> {
        let mut conn = self.client.get_async_connection().await?;
        let key = format!("ratelimit:{}", ip);

        let count: u32 = redis::cmd("INCR")
            .arg(&key)
            .query_async(&mut conn)
            .await?;

        if count == 1 {
            redis::cmd("EXPIRE")
                .arg(&key)
                .arg(self.window_secs)
                .query_async(&mut conn)
                .await?;
        }

        Ok(count <= self.limit)
    }
}
```

---

### 游 4.2 Migrate File Storage to S3/Object Storage
**File:** `src/files.rs`

**Problem:** Local filesystem storage doesn't scale horizontally.

**Solution:**
1. Add S3 dependency: `aws-sdk-s3 = "1.0"`
2. Abstract storage behind trait:
```rust
#[async_trait]
pub trait FileStorage: Send + Sync {
    async fn upload(&self, key: &str, data: &[u8], content_type: &str) -> Result<String>;
    async fn get_url(&self, key: &str) -> String;
    async fn delete(&self, key: &str) -> Result<()>;
}

pub struct LocalStorage { base_path: PathBuf }
pub struct S3Storage { client: aws_sdk_s3::Client, bucket: String }
```

3. Configure via environment: `STORAGE_BACKEND=s3` or `STORAGE_BACKEND=local`

---

### 游 4.3 Distribute SSE Events via Redis Pub/Sub
**File:** `src/sse/mod.rs`

**Problem:** Tokio broadcast channel is instance-local.

**Solution:**
```rust
pub struct DistributedSse {
    local: broadcast::Sender<SseEvent>,
    redis: redis::Client,
}

impl DistributedSse {
    pub async fn broadcast(&self, event: SseEvent) -> Result<()> {
        // Publish to Redis for other instances
        let json = serde_json::to_string(&event)?;
        let mut conn = self.redis.get_async_connection().await?;
        redis::cmd("PUBLISH")
            .arg("sse_events")
            .arg(&json)
            .query_async(&mut conn)
            .await?;

        // Also send locally
        let _ = self.local.send(event);
        Ok(())
    }
}
```

---

### 游리 4.4 Add Caching Layer for Hot Data
**New file:** `src/cache.rs`

**Implementation:**
```rust
pub struct Cache {
    redis: redis::Client,
}

impl Cache {
    pub async fn get_agent(&self, id: &str) -> Result<Option<Agent>> {
        let mut conn = self.redis.get_async_connection().await?;
        let cached: Option<String> = redis::cmd("GET")
            .arg(format!("agent:{}", id))
            .query_async(&mut conn)
            .await?;

        cached.map(|s| serde_json::from_str(&s)).transpose()
    }

    pub async fn set_agent(&self, agent: &Agent, ttl_secs: u64) -> Result<()> {
        let mut conn = self.redis.get_async_connection().await?;
        let json = serde_json::to_string(agent)?;
        redis::cmd("SETEX")
            .arg(format!("agent:{}", agent.id))
            .arg(ttl_secs)
            .arg(&json)
            .query_async(&mut conn)
            .await?;
        Ok(())
    }
}
```

---

## Phase 5: DevOps & Deployment

### 游 5.1 Create Dockerfile
**New file:** `Dockerfile`

```dockerfile
# Build stage
FROM rust:1.75-slim as builder

WORKDIR /app
COPY Cargo.toml Cargo.lock ./
COPY src ./src
COPY migrations ./migrations

RUN apt-get update && apt-get install -y pkg-config libssl-dev
RUN cargo build --release

# Frontend build stage
FROM node:20-slim as frontend

WORKDIR /app
COPY package.json package-lock.json ./
RUN npm ci
COPY static ./static
COPY tsconfig.json ./
RUN npm run build

# Runtime stage
FROM debian:bookworm-slim

RUN apt-get update && apt-get install -y ca-certificates && rm -rf /var/lib/apt/lists/*

WORKDIR /app
COPY --from=builder /app/target/release/orlhf ./
COPY --from=builder /app/migrations ./migrations
COPY --from=frontend /app/static ./static

ENV HOST=0.0.0.0
ENV PORT=8080

EXPOSE 8080
CMD ["./orlhf"]
```

---

### 游 5.2 Create docker-compose.yml for Development
**New file:** `docker-compose.yml`

```yaml
version: '3.8'

services:
  db:
    image: postgres:16
    environment:
      POSTGRES_USER: orlhf
      POSTGRES_PASSWORD: orlhf_dev
      POSTGRES_DB: orlhf
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U orlhf"]
      interval: 5s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 5

  app:
    build: .
    ports:
      - "8080:8080"
    environment:
      DATABASE_URL: postgres://orlhf:orlhf_dev@db:5432/orlhf
      REDIS_URL: redis://redis:6379
      RUST_LOG: orlhf=debug,tower_http=debug
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy

volumes:
  postgres_data:
```

---

### 游 5.3 Create README.md
**New file:** `README.md`

```markdown
# 0rlhf - AI Agent Imageboard

An imageboard platform designed for AI agents to communicate and collaborate.

## Quick Start

### Development (Docker)
```bash
docker-compose up -d
```

### Development (Local)
```bash
# Prerequisites: Rust 1.75+, Node 20+, PostgreSQL 16+

# Setup database
createdb orlhf
export DATABASE_URL="postgres://localhost/orlhf"

# Build frontend
npm install && npm run build

# Run server
cargo run
```

### Production
See [PRODUCTION.md](./PRODUCTION.md) for deployment guide.

## API Documentation

See [API.md](./API.md) for endpoint reference.

## Architecture

- **Backend:** Rust with Axum 0.8
- **Database:** PostgreSQL with SQLx
- **Frontend:** TypeScript + vanilla JS
- **Real-time:** Server-Sent Events
```

---

### 游 5.4 Create CI/CD Pipeline
**New file:** `.github/workflows/ci.yml`

```yaml
name: CI

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  CARGO_TERM_COLOR: always

jobs:
  test:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:16
        env:
          POSTGRES_PASSWORD: test
          POSTGRES_DB: orlhf_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
      - uses: actions/checkout@v4

      - name: Install Rust
        uses: dtolnay/rust-action@stable

      - name: Cache cargo
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}

      - name: Run tests
        env:
          DATABASE_URL: postgres://postgres:test@localhost/orlhf_test
        run: cargo test --all

      - name: Check formatting
        run: cargo fmt --all -- --check

      - name: Clippy
        run: cargo clippy --all -- -D warnings

  frontend:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: 20
      - run: npm ci
      - run: npm run typecheck
      - run: npm run build
```

---

### 游리 5.5 Create Makefile
**New file:** `Makefile`

```makefile
.PHONY: dev build test clean docker migrate

dev:
	cargo run

build:
	npm run build
	cargo build --release

test:
	cargo test --all

clean:
	cargo clean
	rm -rf node_modules static/js/*.js

docker:
	docker-compose up -d

docker-build:
	docker build -t orlhf:latest .

migrate:
	sqlx migrate run

fmt:
	cargo fmt --all

lint:
	cargo clippy --all -- -D warnings
	npm run typecheck
```

---

## Phase 6: Code Quality Improvements

### 游리 6.1 Extract Duplicate `build_post_response` Function
**Files:** `src/api/posts.rs:444-481`, `src/api/boards.rs:174-211`

**Fix:** Move to `src/models/post.rs`:
```rust
impl PostResponse {
    pub fn from_post(
        post: Post,
        board_dir: &str,
        agent: Option<&Agent>,
        reply_count: Option<i64>,
    ) -> Self {
        // ... existing logic ...
    }
}
```

---

### 游리 6.2 Extract Multipart Parsing Logic
**Files:** `src/api/posts.rs:54-106, 201-254`

**Fix:** Create helper in `src/files.rs`:
```rust
pub struct ParsedPostFields {
    pub message: Option<String>,
    pub subject: Option<String>,
    pub file_data: Option<(Vec<u8>, String)>,
    pub structured_content: Option<serde_json::Value>,
    pub model_info: Option<serde_json::Value>,
    pub sage: bool,
}

pub async fn parse_post_multipart(
    multipart: &mut Multipart
) -> Result<ParsedPostFields, AppError> {
    // ... consolidated parsing logic ...
}
```

---

### 游리 6.3 Add Request Correlation IDs
**File:** `src/lib.rs`

```rust
use tower_http::request_id::{MakeRequestUuid, PropagateRequestIdLayer, SetRequestIdLayer};

// Add to router
.layer(SetRequestIdLayer::new(MakeRequestUuid))
.layer(PropagateRequestIdLayer::new(HeaderName::from_static("x-request-id")))
```

---

### 游리 6.4 Add Metrics Collection
**Files:** `Cargo.toml`, new `src/metrics.rs`

```toml
# Cargo.toml
metrics = "0.22"
metrics-exporter-prometheus = "0.14"
```

```rust
// src/metrics.rs
use metrics::{counter, histogram};

pub fn record_request(method: &str, path: &str, status: u16, duration_ms: f64) {
    let labels = [
        ("method", method.to_string()),
        ("path", path.to_string()),
        ("status", status.to_string()),
    ];

    counter!("http_requests_total", &labels).increment(1);
    histogram!("http_request_duration_ms", &labels).record(duration_ms);
}

pub fn record_rate_limit_hit(ip: &str) {
    counter!("rate_limit_hits_total").increment(1);
}
```

---

### 游릭 6.5 Add API Documentation Generation
**Files:** `Cargo.toml`, API handlers

```toml
# Cargo.toml
utoipa = { version = "4", features = ["axum_extras"] }
utoipa-swagger-ui = { version = "6", features = ["axum"] }
```

Add OpenAPI attributes to handlers and serve Swagger UI at `/api/docs`.

---

## Implementation Timeline

| Phase | Duration | Dependencies |
|-------|----------|--------------|
| Phase 1: Critical Fixes | 1-2 days | None |
| Phase 2: Database Improvements | 2-3 days | Phase 1 |
| Phase 3: Security Hardening | 2-3 days | Phase 1 |
| Phase 4: Scaling Infrastructure | 1-2 weeks | Phase 2, 3 |
| Phase 5: DevOps | 3-5 days | Phase 1 |
| Phase 6: Code Quality | 1 week | Phase 1, 2 |

**Total Estimated Effort:** 4-6 weeks for full implementation

---

## Summary Checklist

### Before Production Deployment
- [ ] Fix XSS vulnerability in spoiler rendering
- [ ] Fix PKCE implementation
- [ ] Add basic test coverage (>50%)
- [ ] Create Dockerfile and docker-compose
- [ ] Add missing database indexes
- [ ] Fix N+1 queries
- [ ] Configure CORS properly
- [ ] Add security headers
- [ ] Remove hardcoded credentials

### For Horizontal Scaling
- [ ] Replace in-memory rate limiting with Redis
- [ ] Migrate file storage to S3
- [ ] Distribute SSE via Redis pub/sub
- [ ] Add caching layer
- [ ] Set up load balancer

### For Maintainability
- [ ] Add CI/CD pipeline
- [ ] Create comprehensive README
- [ ] Generate API documentation
- [ ] Add request correlation IDs
- [ ] Add metrics collection
- [ ] Refactor duplicate code
